{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68a0802a",
   "metadata": {},
   "source": [
    "# Fake News Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34865a8e",
   "metadata": {},
   "source": [
    "## Installing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a23a115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "    FAKE NEWS DETECTION - TRAINING WITH TITLE + TEXT COMBINED\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"    FAKE NEWS DETECTION - TRAINING WITH TITLE + TEXT COMBINED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "os.makedirs(\"model\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b507f",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efb35954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/7] Loading datasets...\n",
      "‚úì Fake news: 23,481 articles\n",
      "‚úì True news: 21,417 articles\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[1/7] Loading datasets...\")\n",
    "start_time = time.time()\n",
    "\n",
    "data_fake = pd.read_csv('Datasets/Fake.csv')\n",
    "data_true = pd.read_csv('Datasets/True.csv')\n",
    "\n",
    "print(f\"‚úì Fake news: {len(data_fake):,} articles\")\n",
    "print(f\"‚úì True news: {len(data_true):,} articles\")\n",
    "\n",
    "data_fake[\"class\"] = 0\n",
    "data_true['class'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c382f",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a4cc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/7] Preparing data...\n",
      "‚úì Total samples: 44,878\n",
      "‚úì Fake: 23,471, Real: 21,407\n",
      "\n",
      "[3/7] Preprocessing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:34: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:36: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:39: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:34: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:36: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:39: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:40: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\TUF\\AppData\\Local\\Temp\\ipykernel_9708\\1491438121.py:34: SyntaxWarning: invalid escape sequence '\\['\n",
      "  text = re.sub('\\[.*?\\]', '', text)\n",
      "C:\\Users\\TUF\\AppData\\Local\\Temp\\ipykernel_9708\\1491438121.py:36: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
      "C:\\Users\\TUF\\AppData\\Local\\Temp\\ipykernel_9708\\1491438121.py:39: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  text = re.sub('\\w*\\d\\w*', '', text)\n",
      "C:\\Users\\TUF\\AppData\\Local\\Temp\\ipykernel_9708\\1491438121.py:40: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text = re.sub('\\s+', ' ', text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Preprocessing done in 30.53s\n",
      "‚úì Final dataset: 44,878 articles\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[2/7] Preparing data...\")\n",
    "\n",
    "# Remove last 10 for manual testing\n",
    "data_fake = data_fake[:-10]\n",
    "data_true = data_true[:-10]\n",
    "\n",
    "# Merge datasets\n",
    "data_merge = pd.concat([data_fake, data_true], axis=0)\n",
    "\n",
    "# Keep title, text, and class\n",
    "data = data_merge[['title', 'text', 'class']].copy()\n",
    "\n",
    "# Handle missing values\n",
    "data['title'] = data['title'].fillna('')\n",
    "data['text'] = data['text'].fillna('')\n",
    "\n",
    "# Shuffle\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úì Total samples: {len(data):,}\")\n",
    "print(f\"‚úì Fake: {(data['class']==0).sum():,}, Real: {(data['class']==1).sum():,}\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. TEXT PREPROCESSING\n",
    "# ============================================================\n",
    "print(\"\\n[3/7] Preprocessing text...\")\n",
    "preprocess_start = time.time()\n",
    "\n",
    "def wordopt(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub(\"\\\\W\", \" \", text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Clean both title and text\n",
    "data['title_clean'] = data['title'].apply(wordopt)\n",
    "data['text_clean'] = data['text'].apply(wordopt)\n",
    "\n",
    "# COMBINE TITLE AND TEXT - This is the key change!\n",
    "# Title gets more weight by being mentioned first\n",
    "data['combined'] = data['title_clean'] + ' ' + data['title_clean'] + ' ' + data['text_clean']\n",
    "\n",
    "# Remove empty entries\n",
    "data = data[data['combined'].str.len() > 10]\n",
    "\n",
    "print(f\"‚úì Preprocessing done in {time.time()-preprocess_start:.2f}s\")\n",
    "print(f\"‚úì Final dataset: {len(data):,} articles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1cd410",
   "metadata": {},
   "source": [
    "#### Train And Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "097c2870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/7] Splitting data...\n",
      "‚úì Train: 33,658, Test: 11,220\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[4/7] Splitting data...\")\n",
    "\n",
    "X = data['combined']  # Use combined title+text\n",
    "y = data['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"‚úì Train: {len(X_train):,}, Test: {len(X_test):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfe0118",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe609699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/7] Vectorizing (2-4 minutes)...\n",
      "‚úì Features: 50,000\n",
      "‚úì Time: 32.02s\n",
      "‚úì Vectorizer saved\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[5/7] Vectorizing (2-4 minutes)...\")\n",
    "vec_start = time.time()\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=50000,\n",
    "    min_df=5,\n",
    "    max_df=0.8,\n",
    "    ngram_range=(1, 2),\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "xv_train = vectorizer.fit_transform(X_train)\n",
    "xv_test = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"‚úì Features: {xv_train.shape[1]:,}\")\n",
    "print(f\"‚úì Time: {time.time()-vec_start:.2f}s\")\n",
    "\n",
    "# Save vectorizer\n",
    "joblib.dump(vectorizer, \"model/tfidf_vectorizer.pkl\")\n",
    "print(\"‚úì Vectorizer saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20157149",
   "metadata": {},
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "453dfdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6/7] Training models...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üîÑ Training Gradient Boosting...\n",
      "  Train: 1.0000, Test: 0.9982, Time: 1306.58s\n",
      "  üíæ Saved to model/gradient_boosting_model.pkl\n",
      "\n",
      "üîÑ Training Random Forest...\n",
      "  Train: 1.0000, Test: 0.9947, Time: 7.50s\n",
      "  üíæ Saved to model/random_forest_model.pkl\n",
      "\n",
      "üîÑ Training Decision Tree...\n",
      "  Train: 1.0000, Test: 0.9969, Time: 32.64s\n",
      "  üíæ Saved to model/decision_tree_model.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[6/7] Training models...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "models = {\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(\n",
    "        n_estimators=100, max_depth=10, learning_rate=0.1, random_state=42\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=100, random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(\n",
    "        max_depth=100, random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîÑ Training {name}...\")\n",
    "    t_start = time.time()\n",
    "    \n",
    "    model.fit(xv_train, y_train)\n",
    "    \n",
    "    train_acc = model.score(xv_train, y_train)\n",
    "    test_acc = model.score(xv_test, y_test)\n",
    "    t_time = time.time() - t_start\n",
    "    \n",
    "    print(f\"  Train: {train_acc:.4f}, Test: {test_acc:.4f}, Time: {t_time:.2f}s\")\n",
    "    \n",
    "    # Save model\n",
    "    filename = f\"model/{name.lower().replace(' ', '_')}_model.pkl\"\n",
    "    joblib.dump(model, filename)\n",
    "    print(f\"  üíæ Saved to {filename}\")\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'test_acc': test_acc,\n",
    "        'train_acc': train_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65f82902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[7/7] Summary\n",
      "======================================================================\n",
      "‚è±Ô∏è  Total time: 23.53 minutes\n",
      "\n",
      "üìä Performance:\n",
      "  Gradient Boosting    Test: 0.9982\n",
      "  Random Forest        Test: 0.9947\n",
      "  Decision Tree        Test: 0.9969\n",
      "\n",
      "üèÜ Best: Gradient Boosting (0.9982)\n",
      "\n",
      "======================================================================\n",
      "TESTING FUNCTION\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Testing function ready!\n",
      "\n",
      "üìù Usage:\n",
      "   test_news(\"Breaking News Title\", \"Full article text...\", \"Gradient Boosting\")\n",
      "\n",
      "üí° You can now test with BOTH title and content!\n",
      "\n",
      "======================================================================\n",
      "‚ú® TRAINING COMPLETE! ‚ú®\n",
      "======================================================================\n",
      "\n",
      "üéØ Your models now work with:\n",
      "   1. Just text (if title is empty)\n",
      "   2. Title + text (for better accuracy)\n",
      "   3. Just title (if text is empty, but less accurate)\n",
      "\n",
      "üöÄ Ready for Streamlit deployment!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[7/7] Summary\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"‚è±Ô∏è  Total time: {(time.time()-start_time)/60:.2f} minutes\\n\")\n",
    "\n",
    "print(\"üìä Performance:\")\n",
    "for name, data in results.items():\n",
    "    print(f\"  {name:20} Test: {data['test_acc']:.4f}\")\n",
    "\n",
    "best = max(results.items(), key=lambda x: x[1]['test_acc'])\n",
    "print(f\"\\nüèÜ Best: {best[0]} ({best[1]['test_acc']:.4f})\")\n",
    "\n",
    "# ============================================================\n",
    "# TESTING FUNCTION\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TESTING FUNCTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def test_news(title, text, model_name=\"Gradient Boosting\"):\n",
    "    \"\"\"\n",
    "    Test news with title and text\n",
    "    \n",
    "    Args:\n",
    "        title: News title (can be empty string)\n",
    "        text: News content\n",
    "        model_name: Which model to use\n",
    "    \"\"\"\n",
    "    # Clean inputs\n",
    "    title_clean = wordopt(title)\n",
    "    text_clean = wordopt(text)\n",
    "    \n",
    "    # Combine same way as training (title appears twice for emphasis)\n",
    "    combined = f\"{title_clean} {title_clean} {text_clean}\"\n",
    "    \n",
    "    # Vectorize\n",
    "    vec = vectorizer.transform([combined])\n",
    "    \n",
    "    # Predict\n",
    "    model = results[model_name]['model']\n",
    "    pred = model.predict(vec)[0]\n",
    "    \n",
    "    result = \"FAKE NEWS ‚ùå\" if pred == 0 else \"REAL NEWS ‚úÖ\"\n",
    "    \n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        proba = model.predict_proba(vec)[0]\n",
    "        print(f\"\\nüîç Model: {model_name}\")\n",
    "        print(f\"üì∞ Prediction: {result}\")\n",
    "        print(f\"üìä Confidence: Fake={proba[0]:.1%}, Real={proba[1]:.1%}\")\n",
    "    else:\n",
    "        print(f\"\\nüîç Model: {model_name}\")\n",
    "        print(f\"üì∞ Prediction: {result}\")\n",
    "    \n",
    "    return pred\n",
    "\n",
    "print(\"\\n‚úÖ Testing function ready!\")\n",
    "print(\"\\nüìù Usage:\")\n",
    "print('   test_news(\"Breaking News Title\", \"Full article text...\", \"Gradient Boosting\")')\n",
    "print(\"\\nüí° You can now test with BOTH title and content!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚ú® TRAINING COMPLETE! ‚ú®\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüéØ Your models now work with:\")\n",
    "print(\"   1. Just text (if title is empty)\")\n",
    "print(\"   2. Title + text (for better accuracy)\")\n",
    "print(\"   3. Just title (if text is empty, but less accurate)\")\n",
    "print(\"\\nüöÄ Ready for Streamlit deployment!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
